import itertools
import os

import numpy as np
import lightgbm as lgb
import pickle
import pandas as pd
import datetime as dt

from examples.simulation_utils.dataset_game import DatasetGame
from examples.simulation_utils.utils import generate_probabilities_matrix


class EMBERClassifierGame(DatasetGame):

    def __init__(self, action_set: list, measurement_set: list, opponent_action_sequence, raw_measurements,
                 measurement_sequence, finite_measurements: bool, type1_weight=1., type2_weight=1.):
        """Create game. The goal is to minimize the cost := type1_weight*p(false pos) + type2_weight*p(false neg)

        Args:
            type1_weight (float): the weight for false positive rates
            type2_weight (float): the weight for false negative rates
        """
        super().__init__(action_set=action_set, measurement_set=measurement_set, raw_measurements=raw_measurements,
                         opponent_action_sequence=opponent_action_sequence,
                         measurement_sequence=measurement_sequence, finite_measurements=finite_measurements)
        self.type1_weight = type1_weight
        self.type2_weight = type2_weight

    def cost(self, p1_action, p2_action) -> float:
        """Returns game outcome

        Args:
            p1_action: action for player 1
            p2_action: action for player 2

        Returns:
            game result (float): A*false_pos + B*false_neg
        """
        # while in principle we care about the probability computed over time, the cost can be written as a stage cost
        # as total_cost^T = A*\sum_{t=0}^T I(a_1^t=1, a_2^t=0)/T + B*\sum_{t=0}^T I(a_1^t=0, a_2^t=1)/T
        #               = A*\sum_{t=0}^{T-1} I(a_1^t=1, a_2^t=0)/T + B*\sum_{t=0}^{T-1} I(a_1^t=0, a_2^t=1)/T +
        #                 A*I(a_1^T=1, a_2^T=0)/T + B*I(a_1^T=0, a_2^T=1)/T
        #               = A*I(a_1^T=1, a_2^T=0)/T + B*I(a_1^T=0, a_2^T=1)/T + total_cost^{T-1}*(T-1)/T
        false_pos = (p1_action == 1) and (p2_action == 0)
        false_neg = (p1_action == 0) and (p2_action == 1)
        return self.type1_weight * false_pos + self.type2_weight * false_neg


def find_optimal_threshold(y_true: np.array, y_pred_proba: np.array, w_fp: float, w_fn: float):
    """
    Finds the threshold that minimizes the weighted cost w_fp * P(false pos) + w_fn * P(false neg) on a grid search
    Args:
        y_true (np.array): the true labels
        y_pred_proba (np.array): the prediction probabilities from a classifier that we want to implement a threshold for
        w_fp (float): the weight for false positive errors
        w_fn (float): the weight for false negative errors

    Returns: (float) the best threshold

    """
    thresholds = np.linspace(0, 1, 10000)
    best_threshold = thresholds[0]
    min_cost = float('inf')

    for T in thresholds:
        false_positive_rate = np.mean((y_pred_proba >= T) & (y_true == 0))
        false_negative_rate = np.mean((y_pred_proba < T) & (y_true == 1))

        total_cost = w_fp * false_positive_rate + w_fn * false_negative_rate
        if total_cost < min_cost:
            min_cost = total_cost
            best_threshold = T
    print(f'Optimal threshold={best_threshold:.2f} for error weights I:{w_fp} II:{w_fn}')

    return best_threshold


def load_ember_data(ember_data_path: str):
    """
    Load the EMBER dataset
    Args:
        ember_data_path (str): folder containing the pickled ember data

    Returns: dict[str, np.ndarray]

    """
    data = dict(x_train=np.array([]), x_test=np.array([]), y_train=np.array([]), y_test=np.array([]))
    for key, _ in data.items():
        with open(ember_data_path + f'{key}.pkl', 'rb') as f:
            data[key] = pickle.load(f)
    return data


def ember_classifier(ember_data, n_train: int,
                     type1_weight: float, type2_weight: float, base_model_file: str) -> tuple:
    """
    Prepares the data from EMBER to be streamed in "real-time" fashion. First the data is loaded, then sorted by time-
    stamp, and finally LightGBM models are trained on n_train points to generate the stationary models. n_test points
    are stored in arrays with raw features, predictions, and ground-truth labels for the purposes of testing. The
    performance of the LightGBM models is stored in a file with the base path: base_model_file
    Args:
        n_train (int): the number of training points
        n_test (int): the number of test points
        type1_weight (float): the weight associated with false positives
        type2_weight (float): the weight associated with false negatives
        base_model_file (str): the file path for the models to be persisted to

    Returns: (tuple): measurement_sequence (np.ndarray), x_test (np.ndarray), y_test (np.ndarray), \
        threshold (float), outcomes (list[str]), error_df (pd.DataFrame)

    """

    data = pd.DataFrame(np.vstack((ember_data['x_train'], ember_data['x_test'])))
    data['y'] = np.hstack((ember_data['y_train'], ember_data['y_test'])).T
    # load metadata about dates
    meta_df = pd.read_json('../data/extracted_data.jsonl', lines=True)
    data['appeared'] = pd.to_datetime(meta_df['appeared'])
    # only include the labeled points
    data = data[data['y'] >= 0.]

    # sort based on the timestamp
    data['timestamp'] = pd.to_datetime(data.iloc[:, 626], unit='s')
    # only use recently appeared samples (i.e. in the past two years)
    time_mask = (data['timestamp'] >= dt.datetime(2016, 1, 1)) & (data['timestamp'] <= dt.datetime(2019, 1, 1))
    data = data[time_mask]
    # ensure none of the dates are more than a month in the future
    forward_mask = data['appeared'] + dt.timedelta(days=30) >= data['timestamp']
    data = data[forward_mask]
    data.sort_values('timestamp', inplace=True)

    y_total = data['y'].values
    data.drop(['y', 'timestamp', 'appeared'], axis='columns', inplace=True)
    x_total = data.values
    time_array = (x_total[:, 626] / 86400).round() - x_total[:, 626].min()  # time in days
    # break into train and test sets
    x_train = x_total[:n_train]
    x_test = x_total[n_train:]
    # scale the features (not strictly necessary)
    x_train, x_scale = normalize_features(x_train)
    x_test, _ = normalize_features(x_test, x_scale)

    measurement_sequences = []
    y_train = y_total[:n_train]
    y_test = y_total[n_train:]
    time_array = time_array[n_train:] - time_array[n_train]  # make the time start at zero
    if len(y_train) < n_train:
        raise IndexError('Not enough points in training set')

    params_paper = {
        "boosting": "gbdt",
        "objective": "binary",
        "num_iterations": 1000,
        "learning_rate": 0.05,
        "num_leaves": 2048,
        # "num_leaves": 31,
        "max_depth": 15,
        "min_data_in_leaf": 50,
        "feature_fraction": 0.5
    }

    params_shallow = {
        "boosting": "gbdt",
        "objective": "binary",
        "num_iterations": 500,
        "learning_rate": 0.05,
        "num_leaves": 50,
        # "num_leaves": 31,
        "max_depth": 10,
        "min_data_in_leaf": 20,
        "feature_fraction": 0.5
    }

    params_rf = {
        "boosting": "rf",
        "objective": "binary",
        "num_iterations": 1000,
        "learning_rate": 0.05,
        "num_leaves": 10,
        "max_depth": 5,
        "min_data_in_leaf": 10,
        "feature_fraction": 0.5
    }

    train_preds = []
    for i, params in zip(range(n_classifiers), [params_paper, params_rf, params_shallow]):
        x_train_sub = x_train
        x_test_sub = x_test
        if i == 1:  # always run the paper model first and then take most important features
            feat_importance = np.vstack((np.arange(2381), lgbm_model.feature_importance())).T
            feat_importance = feat_importance[feat_importance[:, 1].argsort()]
            x_train_sub = x_train[:, feat_importance[-500:, 0]]
            x_test_sub = x_test[:, feat_importance[-500:, 0]]

        train = lgb.Dataset(x_train_sub, y_train)
        file = f'../data/ember_short_unit_{n_train}_short_model_{i}' + '.txt'
        if os.path.isfile(file):
            lgbm_model = lgb.Booster(model_file=file)
        else:
            lgbm_model = lgb.train(params, train)
            # save the model
            lgbm_model.save_model(file)

        train_preds.append(lgbm_model.predict(x_train_sub))
        measurement_sequences.append(lgbm_model.predict(x_test_sub))

    p2_act_sequence = y_test

    error_df = pd.DataFrame.from_dict({'pred_prob_{}'.format(i): measurement_sequences[i] for i in range(n_classifiers)})

    indep_probs = np.vstack(measurement_sequences).T
    outcomes, measurement_sequence = generate_probabilities_matrix(indep_probs)
    # classify based on a threshold that takes into account type1 and type2 errors
    mean_pred = np.vstack(train_preds).T.mean(axis=1)
    threshold = find_optimal_threshold(y_train, mean_pred, type1_weight, type2_weight)
    mean_pred_test = np.vstack(measurement_sequences).T.mean(axis=1)

    def threshold_and_cost(probs, tau):
        classifier_predictions = (probs > tau).astype(int)
        fp = (classifier_predictions == 1) & (p2_act_sequence == 0)
        fn = (classifier_predictions == 0) & (p2_act_sequence == 1)
        return type1_weight * fp + type2_weight * fn

    for tau in [0.25, 0.5, 0.75]:
        error_df['cost_{}'.format(tau)] = threshold_and_cost(mean_pred_test, tau)
        error_df['cost_c1_{}'.format(tau)] = threshold_and_cost(measurement_sequences[0], tau)

    error_df['label'] = p2_act_sequence
    error_df['timestamp'] = x_test[:, 626]*x_scale['x_range'][626] + x_scale['x_min'][626]
    for i in range(n_classifiers):
        error_df['mae_{}'.format(i)] = np.abs(error_df['pred_prob_{}'.format(i)] - error_df['label'])

    with open(base_model_file + f'{type1_weight}_{type2_weight}_short_classifier.pkl', 'wb') as f:
        pickle.dump(error_df, f)

    return measurement_sequence, x_test, y_test, \
        threshold, outcomes, error_df, indep_probs, time_array


def normalize_features(x: np.ndarray, x_scale: dict=None) -> tuple[np.ndarray, dict[str, np.ndarray]]:
    """
    Scale the feature space to be [0,1] for the training set and use the same scale factors for the test set
    Args:
        x (np.ndarray): the data to scale
        x_scale (dict, optional): the min values and range

    Returns: (np.array, dict[str, np.ndarray])

    """
    if x_scale is None:
        x_min = x.min(axis=0)
        x_range = x.max(axis=0) - x_min
        x_range[x_range == 0] = 1.
    else:
        x_min = x_scale['x_min']
        x_range = x_scale['x_range']

    x_norm = (x - x_min) / x_range
    return x_norm, dict(x_min=x_min, x_range=x_range)


def name_file(use_raw_measurement, measurement_to_label, store_energy, unit_step):
    """Helper func to name files uniquely based on test parameters"""
    if unit_step:
        step = '_unit'
    else:
        step = ''
    base_model_file = f'../data/ember_short{step}_{n_train}'
    raw_str = 'nonraw'
    if use_raw_measurement:
        raw_str = 'raw'
    mapping_str = 'ya2j'
    if measurement_to_label:
        mapping_str = 'y2l'
    energy_str = 'noenergy'
    if store_energy:
        energy_str = 'wenergy'
    total_file = base_model_file + f'_{type1}_{type2}_{mapping_str}_{raw_str}_{energy_str}'
    if not os.path.isdir(total_file):
        os.mkdir(total_file)
    return base_model_file, total_file


if __name__ == '__main__':

    from sklearn import svm, neural_network

    from learning_games import LearningGame
    from examples.benchmark_methods import SklearnModel
    from examples.simulation_utils.utils import GamePlay

    ember_filepath: str = '../ember_data/'  # wherever you stored the ember dataset
        # (assumes files are x_train.pkl, y_train.pkl, x_test.pkl, y_test.pkl)
    n_train: int = 50_000  # the number of training points
    m_iter: int = 20_000  # the printout frequency from game play
    type1: float = 1.  # the weight for false pos
    type2: float = 1.  # the weight for false neg
    store_energy = False  # True: stores the energy and persists with rest of simulation data but is memory intensive
    unit_step = True
    # specify beta and lambda values to search over. We later compute the cartesian product of these lists/sets.
    lambda_values: list = [1e2, 1e1, 1e0, 1e-1, 1e-2, 1e-3]
    beta_values: list = [1e0, 1e1, 1e2]

    # parameters for the benchmark models
    use_raw_measurement: bool = True  # whether to use raw features (True) or the probabilistic classifier as input
    # whether to learn a map from measurements to label (True) or estimate the cost associated with each action (False)
    measurement_to_label: bool = False
    # the window size H of recent data
    data_window: int = 10_000
    # the frequency at which the model is updated. More updates requires more time
    update_freq: int = 10_000
    # the mapping from labels to actions when learning measurement_to_label
    label_action_policy = {0: 0, 1: 1}
    # hidden layer sizes for NN/MLP
    hidden_layer_sizes = (50, 50)
    # the max number of training iterations for the NN
    max_train_iter = 1000
    # kernel choice for the SVM
    svm_kernel = 'rbf'
    # random state for the sklearn models
    random_state = 1

    # Not easy to change parameters. Likely require some downstream changes
    n_classifiers: int = 3  # currently can't be changed without modifying how the classifiers are generated
    finite_meas: bool = False  # whether the use finite or infinite measurements

    base_model_file, total_file = name_file(measurement_to_label=measurement_to_label,
                                            use_raw_measurement=use_raw_measurement,
                                            store_energy=store_energy, unit_step=unit_step)

    dataset = load_ember_data(ember_filepath)

    meas_sequence, raw_measurements, p2_act_sequence, \
        classifier_threshold, outcomes_set, classifier_perf, c_probs, time_array = ember_classifier(ember_data=dataset,
                                                                               n_train=n_train,
                                                              base_model_file=base_model_file,
                                                              type1_weight=type1, type2_weight=type2)
    M = len(time_array)

    print('Measurement set: {}'.format(str(outcomes_set)))
    game = EMBERClassifierGame(opponent_action_sequence=p2_act_sequence,
                               measurement_sequence=meas_sequence, measurement_set=outcomes_set,
                               raw_measurements=raw_measurements,
                               action_set=[0, 1], type1_weight=type1, type2_weight=type2,
                               finite_measurements=finite_meas)
    dm = []
    for l, b in itertools.product(lambda_values, beta_values):
        lg = LearningGame(game.action_set, measurement_set=game.measurement_set, finite_measurements=finite_meas,
                          decay_rate=l, inverse_temperature=b, seed=0)
        lg.reset()
        dm.append(lg)

    if measurement_to_label:
        nn_model = neural_network.MLPClassifier(random_state=random_state,
                                                max_iter=max_train_iter,
                                                hidden_layer_sizes=hidden_layer_sizes)
        svm_model = svm.SVC(kernel=svm_kernel)
    else:
        nn_model = neural_network.MLPRegressor(random_state=random_state,
                                               max_iter=max_train_iter,
                                               hidden_layer_sizes=hidden_layer_sizes)
        svm_model = svm.SVR(kernel=svm_kernel)
    svm = SklearnModel(window_size=data_window, action_set=game.action_set, measurement_set=game.measurement_set,
                       raw_measurement=use_raw_measurement, measurement_to_label=measurement_to_label,
                       finite_measurement=finite_meas, policy_map=label_action_policy,
                       update_frequency=update_freq, model=svm_model)

    mlp = SklearnModel(window_size=data_window, action_set=game.action_set, measurement_set=game.measurement_set,
                       raw_measurement=use_raw_measurement, measurement_to_label=measurement_to_label,
                       finite_measurement=finite_meas, policy_map=label_action_policy,
                       update_frequency=update_freq, model=nn_model)
    dm = []
    # dm.append(mlp)
    dm.append(svm)
    # dm.reverse()

    if unit_step:  # use the indexing for the time range
        time_array = None
    # Construct the harness to play the games
    gp = GamePlay(decision_makers=dm,
                  game=game,
                  horizon=M,
                  disp_results_per_iter=m_iter,
                  store_energy_hist=store_energy,
                  time_index=time_array)
    # play all the games
    gp.play_games(save_to=total_file)
